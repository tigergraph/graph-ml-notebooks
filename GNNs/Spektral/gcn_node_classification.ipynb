{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yKmvewGBZ_i"
   },
   "source": [
    "# Graph Convolutional Network\n",
    "\n",
    "This notebook demonstrates the training of [Graph Convolutional Networks (GCN)](https://arxiv.org/pdf/1609.02907.pdf) with TigerGraph. We implement a GCN using the [Spektral](https://graphneural.network/) GNN framework. We train the model on the Cora dataset with TigerGraph as the data store. The dataset contains 2708 machine learning papers and 10556 citation links between the papers. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from a dictionary. The dictionary consists of 1433 unique words. Each paper is classified into one of seven classes based on the topic. The goal is to predict the class of each vertex in the graph.\n",
    "\n",
    "The following libraries are required to run this notebook. Uncomment to install them if necessary. You might need to restart the kernel after installing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ah5LXQByBZ_j",
    "outputId": "70aa0d5c-7159-4fd3-83af-159dc4e32e1a"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#!pip install spektral==1.2.0\n",
    "#!pip install pyTigerGraph[gds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VIDMjl7BZ_k"
   },
   "source": [
    "## Table of Contents\n",
    "* [Data Processing](#data_processing)  \n",
    "* [Train on whole graph](#train_whole)  \n",
    "* [Train on neighborhood subgraphs](#train_subgraph) \n",
    "* [Inference](#inference) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIv7HAB3BZ_l"
   },
   "source": [
    "## Data Processing <a name=\"data_processing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P81VZX1iBZ_l"
   },
   "source": [
    "### Connect to TigerGraph\n",
    "\n",
    "The `TigerGraphConnection` class represents a connection to the TigerGraph database. Under the hood, it stores the necessary information to communicate with the database. It is able to perform quite a few database tasks. Please see its [documentation](https://docs.tigergraph.com/pytigergraph/current/intro/) for details. \n",
    "\n",
    "To connect your database, modify the `config.json` file accompanying this notebook. Set the value of `getToken` based on whether token auth is enabled for your database. Token auth is always enabled for tgcloud databases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9l7RRfLBZ_l"
   },
   "outputs": [],
   "source": [
    "from pyTigerGraph import TigerGraphConnection\n",
    "import json\n",
    "\n",
    "# Read in DB configs\n",
    "with open('../../config.json', \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "    \n",
    "conn = TigerGraphConnection(\n",
    "    host=config[\"host\"],\n",
    "    username=config[\"username\"],\n",
    "    password=config[\"password\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyTigerGraph.datasets import Datasets\n",
    "\n",
    "dataset = Datasets(\"Cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.ingestDataset(dataset, getToken=config[\"getToken\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyTigerGraph.visualization import drawSchema\n",
    "\n",
    "drawSchema(conn.getSchema(force=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJ9Rsl1uBZ_n",
    "outputId": "cbab63cd-640d-47d2-9abb-faf0935a7bb7"
   },
   "outputs": [],
   "source": [
    "conn.getVertexCount('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nEkoNwkDBZ_n",
    "outputId": "ca2d545f-cdfb-4597-a0a6-301d5783d08b"
   },
   "outputs": [],
   "source": [
    "conn.getEdgeCount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2S3jfS2jBZ_o"
   },
   "source": [
    "### Train/validation/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHS6tn2JBZ_o"
   },
   "outputs": [],
   "source": [
    "# The code in this cell is commented out because there is no need to split the vertices into \n",
    "# training/validation/test sets, as the split is already done in the original dataset. \n",
    "# See notebook 1_data_processing for examples on the split function.\n",
    "\n",
    "#split = conn.gds.vertexSplitter(train_mask=0.8, val_mask=0.1, test_mask=0.1)\n",
    "#split.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RvLliqHpBZ_p",
    "outputId": "dcc828da-c86a-4f90-cb0c-164db1b04dcb"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Number of vertices in training set:\",\n",
    "    conn.getVertexCount(\"Paper\", where=\"train_mask!=0\"),\n",
    ")\n",
    "print(\n",
    "    \"Number of vertices in validation set:\",\n",
    "    conn.getVertexCount(\"Paper\", where=\"val_mask!=0\"),\n",
    ")\n",
    "print(\n",
    "    \"Number of vertices in test set:\", \n",
    "    conn.getVertexCount(\"Paper\", where=\"test_mask!=0\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Mr4C8r8BZ_p"
   },
   "source": [
    "## Train on whole graph <a name=\"train_whole\"></a>\n",
    "We first train the model on the whole graph. This will **NOT** work when the graph is large. See the section of training on subgraphs for real use. However, we still include this example for illustration purpose. Hyperparameters for the model and training environment are defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Or16LaCSBZ_p"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hp = {\"hidden_dim\": 64, \n",
    "      \"num_layers\": 2, \n",
    "      \"dropout\": 0.6,\n",
    "      \"lr\": 0.001, \n",
    "      \"l2_penalty\": 5e-4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lbnl3PUnBZ_q"
   },
   "source": [
    "### Construct graph loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3k6JP5yYBZ_q"
   },
   "source": [
    "The `GraphLoader` can get the whole graph from database all at once (`num_batches=1`). See the tutorial on dataloaders for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PLb00KaBZ_q"
   },
   "outputs": [],
   "source": [
    "graph_loader = conn.gds.graphLoader(\n",
    "    v_in_feats=[\"x\"],\n",
    "    v_out_labels=[\"y\"],\n",
    "    v_extra_feats=[\"train_mask\", \"val_mask\", \"test_mask\"],\n",
    "    num_batches=1,\n",
    "    output_format=\"spektral\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OttlUJHBZ_q",
    "outputId": "1e60511e-5270-4e6d-c828-5bffe9477e79"
   },
   "outputs": [],
   "source": [
    "# Get the whole graph from the loader in PyG format\n",
    "data = graph_loader.data\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCpZ3zLUmxTW"
   },
   "outputs": [],
   "source": [
    "x, adj, y, mask_tr, mask_va, mask_te = data.x, data.A, data.y, data.train_mask, data.val_mask, data.test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUdVtiLGBZ_q"
   },
   "source": [
    "### Construct model and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17XFCS0QBZ_q"
   },
   "source": [
    "We build a GCN model with 2 convolutional layers, and use the Adam optimizer with a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASrN-H_EBZ_r"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from spektral.layers import GCNConv\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dropout, Input\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from spektral.layers import GraphSageConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orl-5y_1BZ_r",
    "outputId": "87184a33-20a8-4094-8322-2e936a7b8f5a"
   },
   "outputs": [],
   "source": [
    "device = tf.device(\"GPU\" if tf.config.list_physical_devices('GPU') else \"CPU\")\n",
    "\n",
    "# Model definition\n",
    "x_in = Input(shape=(data.n_node_features,))\n",
    "a_in = Input(shape=(None,), sparse=True)\n",
    "x_1 = GCNConv(hp[\"hidden_dim\"], activation=\"relu\")([x_in, a_in])\n",
    "x_1 = Dropout(hp[\"dropout\"])(x_1)\n",
    "x_2 = GCNConv(7, activation=\"softmax\")([x_1, a_in])\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=[x_in, a_in], outputs=x_2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4P60Ldknbto"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=hp[\"lr\"])\n",
    "loss_fn = CategoricalCrossentropy()\n",
    "\n",
    "one_hot_y = to_categorical(y, num_classes=7)\n",
    "tf_a = tf.SparseTensor(#converts the scipy sparse matrix to a tensorflow sparse matrix\n",
    "    indices=np.array([adj.row, adj.col]).T,\n",
    "    values=adj.data,\n",
    "    dense_shape=adj.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OCi5bF0BZ_s"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJ0-OimpBZ_s"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Br5-2gCWUVh1"
   },
   "outputs": [],
   "source": [
    "# Training step\n",
    "@tf.function\n",
    "def train():\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model([x, tf_a], training=True)\n",
    "        loss = loss_fn(tf.boolean_mask(one_hot_y, mask_tr), tf.boolean_mask(predictions, mask_tr))\n",
    "        loss += sum(model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXMFhmM8UWBm"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def evaluate():\n",
    "    predictions = model([x, tf_a], training=False)\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for mask in [mask_tr, mask_va, mask_te]:\n",
    "        loss = loss_fn(tf.boolean_mask(one_hot_y, mask), tf.boolean_mask(predictions, mask))\n",
    "        loss += sum(model.losses)\n",
    "        losses.append(loss)\n",
    "        acc = tf.reduce_mean(categorical_accuracy(tf.boolean_mask(one_hot_y, mask), tf.boolean_mask(predictions, mask)))\n",
    "        accuracies.append(acc)\n",
    "    return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apXSaEapUYkW",
    "outputId": "5937f1a9-fec3-4bf3-92d8-2109dd0d9fe1"
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train()\n",
    "    l, a = evaluate()\n",
    "    print(\n",
    "        \"Epoch {:.0f}:\\n\"\n",
    "        \"Training Loss: {:.4f}, Training Accuracy: {:.4f}, \"\n",
    "        \"Validation Loss: {:.4f}, Validation Accuracy: {:.4f}\".format(epoch, l[0], a[0], l[1], a[1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b221-7HaBZ_t"
   },
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fOw52gLZVAVK",
    "outputId": "8d0fe890-7780-4682-c928-7d090cdaa016"
   },
   "outputs": [],
   "source": [
    "l, a = evaluate()\n",
    "\n",
    "print(\"Testing Loss: {:.4f}, Testing Accuracy: {:.4f}\".format(l[2], a[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afw7ozPEBZ_t"
   },
   "source": [
    "## Train on Neighborhood Subgraphs <a name=\"train_subgraph\"></a>\n",
    "Alternatively, we train the model on the neighborhood subgraphs. Each subgraph contains the 2 hop neighborhood of certain seed vertices. This method  will allow us to train the model on graphs that are way larger than the CORA dataset because we don't load the whole graph into memory all at once. \n",
    "\n",
    "We will use the same parameters as before, but we will use the NeighborLoader to load subgraphs. Once we finish iterating over all the subgraphs generated by the loader, it is guaranteed to cover all vertices in the graph (except for those filtered by a user provided mask). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Eyj0DInBZ_t"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hp = {\"batch_size\": 64, \n",
    "      \"num_neighbors\": 10, \n",
    "      \"num_hops\": 2, \n",
    "      \"hidden_dim\": 64,\n",
    "      \"num_layers\": 2, \n",
    "      \"dropout\": 0.6, \n",
    "      \"lr\": 0.01, \n",
    "      \"l2_penalty\": 5e-4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmPjGWx8BZ_t"
   },
   "source": [
    "### Construct neighborhood subgraph loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F-2vvwdBZ_t"
   },
   "source": [
    "Here we construct 3 subgraph loaders. The `train_loader` only uses vertices in the training set as seeds, the `valid_loader` only uses vertices in the validation set, and the `test_loader` only uses vertices in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bslf8ab9BZ_u"
   },
   "outputs": [],
   "source": [
    "train_loader = conn.gds.neighborLoader(\n",
    "    v_in_feats=[\"x\"],\n",
    "    v_out_labels=[\"y\"],\n",
    "    v_extra_feats=[\"train_mask\",\"val_mask\",\"test_mask\"],\n",
    "    output_format=\"spektral\",\n",
    "    batch_size=hp[\"batch_size\"],\n",
    "    num_neighbors=hp[\"num_neighbors\"],\n",
    "    num_hops=hp[\"num_hops\"],\n",
    "    shuffle=True,\n",
    "    filter_by=\"train_mask\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UM7HivbNBZ_u"
   },
   "outputs": [],
   "source": [
    "valid_loader = conn.gds.neighborLoader(\n",
    "    v_in_feats=[\"x\"],\n",
    "    v_out_labels=[\"y\"],\n",
    "    v_extra_feats=[\"train_mask\",\"val_mask\",\"test_mask\"],\n",
    "    output_format=\"spektral\",\n",
    "    batch_size=hp[\"batch_size\"],\n",
    "    num_neighbors=hp[\"num_neighbors\"],\n",
    "    num_hops=hp[\"num_hops\"],\n",
    "    shuffle=False,\n",
    "    filter_by=\"val_mask\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qWFudHHBZ_u"
   },
   "source": [
    "### Construct model and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNX49zi7BZ_u"
   },
   "source": [
    "We build a graphSAGE model with 2 convolutional layers, and use the Adam optimizer with a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PD2O9IxGBZ_u",
    "outputId": "488536a8-e77a-4575-93e6-b6023980338c"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[x_in, a_in], outputs=x_2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0vS73lQvVnHJ"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=hp[\"lr\"])\n",
    "loss_fn = CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo9mZCL8BZ_u"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oboTHGIHBZ_u"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmtxPbLJYhtb"
   },
   "outputs": [],
   "source": [
    "def preprocess_batch(graph):\n",
    "    x, adj, y, mask_tr, mask_va, mask_te = graph.x, graph.A, graph.y, graph.train_mask, graph.val_mask, graph.test_mask\n",
    "    one_hot_y = to_categorical(y, num_classes=7)\n",
    "    tf_a = tf.SparseTensor(#converts the scipy sparse matrix to a tensorflow sparse matrix\n",
    "        indices=np.array([adj.row, adj.col]).T,\n",
    "        values=adj.data,\n",
    "        dense_shape=adj.shape)\n",
    "    return x, tf_a, one_hot_y\n",
    "\n",
    "val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "val_loss_metric = tf.keras.metrics.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shtEf9eSqaIW"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(x, tf_a, one_hot_y, mask):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model([x, tf_a], training=True)\n",
    "        loss = loss_fn(tf.boolean_mask(one_hot_y, mask), tf.boolean_mask(predictions, mask))\n",
    "        acc = tf.reduce_mean(categorical_accuracy(tf.boolean_mask(one_hot_y, mask), tf.boolean_mask(predictions, mask)))\n",
    "        loss += sum(model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6KTqhojqbFC"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(x, tf_a, one_hot_y, mask, metrics=[]):\n",
    "    val_logits = model([x, tf_a], training=False)\n",
    "    for metric in metrics:\n",
    "      metric.update_state(tf.boolean_mask(one_hot_y, mask), tf.boolean_mask(val_logits, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FC5BkosQBZ_v",
    "outputId": "52dd304e-8404-4cfa-c724-6a0f039901c5"
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    for bid, batch in enumerate(train_loader):\n",
    "        batchsize = batch.n_nodes\n",
    "        x, tf_a, one_hot_y = preprocess_batch(batch)\n",
    "        loss, acc = train(x, tf_a, one_hot_y, batch.train_mask)\n",
    "        print(\"Epoch {}, Train Batch {}, Loss {:.4f}, Accuracy {:.4f}\".format(epoch, bid, loss, acc))\n",
    "    for batch in valid_loader:\n",
    "        x, tf_a, one_hot_y = preprocess_batch(batch)\n",
    "        test_step(x, tf_a, one_hot_y, batch.val_mask, metrics = [val_acc_metric, val_loss_metric])\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_loss = val_loss_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    val_loss_metric.reset_states()\n",
    "    print(\"Epoch {}, Valid Loss {:.4f}, Valid Accuracy {:.4f}\".format(epoch, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bt56SLbBBZ_w"
   },
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJ3uEMjqBZ_w"
   },
   "outputs": [],
   "source": [
    "test_loader = conn.gds.neighborLoader(\n",
    "    v_in_feats=[\"x\"],\n",
    "    v_out_labels=[\"y\"],\n",
    "    v_extra_feats=[\"train_mask\",\"val_mask\",\"test_mask\"],\n",
    "    output_format=\"spektral\",\n",
    "    batch_size=hp[\"batch_size\"],\n",
    "    num_neighbors=hp[\"num_neighbors\"],\n",
    "    num_hops=hp[\"num_hops\"],\n",
    "    shuffle=False,\n",
    "    filter_by=\"test_mask\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80ZVgKH1BZ_w",
    "outputId": "ad5c79be-b412-4cde-dabb-74690f54808b"
   },
   "outputs": [],
   "source": [
    "acc = tf.keras.metrics.CategoricalAccuracy()\n",
    "for batch in test_loader:\n",
    "    x, tf_a, one_hot_y = preprocess_batch(batch)\n",
    "    test_step(x, tf_a, one_hot_y, batch.val_mask, metrics = [acc])\n",
    "print(\"Accuracy: {:.4f}\".format(acc.result()))\n",
    "acc.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QS2GiqhBZ_w"
   },
   "source": [
    "## Inference <a name=\"inference\"></a>\n",
    "\n",
    "Finally, we use the trained model for node classification. At this stage, we typically do inference/prediction for specific nodes instead of random batches, so we will create a new data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LI6p7kGvBZ_w"
   },
   "outputs": [],
   "source": [
    "infer_loader = conn.gds.neighborLoader(\n",
    "    v_in_feats=[\"x\"],\n",
    "    v_out_labels=[\"y\"],\n",
    "    v_extra_feats=[\"train_mask\",\"val_mask\",\"test_mask\"],\n",
    "    output_format=\"spektral\",\n",
    "    num_neighbors=hp[\"num_neighbors\"],\n",
    "    num_hops=hp[\"num_hops\"],\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6p41oTYfBZ_w"
   },
   "outputs": [],
   "source": [
    "# Fetch specific nodes by their IDs and do prediction. \n",
    "# Each node is represented by a dict with two mandatory keys: primary_id and type.\n",
    "input_nodes = [{\"primary_id\": 7, \"type\": \"Paper\"}, \n",
    "               {\"primary_id\": 999, \"type\": \"Paper\"}]\n",
    "data = infer_loader.fetch(input_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FkyHxbzvBZ_w",
    "outputId": "09085187-806a-4f20-c1c7-a32198510183"
   },
   "outputs": [],
   "source": [
    "# The returned data are the neighborhood subgraphs of the input nodes.\n",
    "# The original IDs of the nodes in the subgraphs are stored in the \n",
    "# `primary_id` attribute.\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExEER-TZBZ_w",
    "outputId": "0597120b-538f-4040-9522-a287748f731b"
   },
   "outputs": [],
   "source": [
    "# Predict. Predictions for both the input nodes and others in their \n",
    "# neighborhoods are generated.\n",
    "x, tf_a, one_hot_y = preprocess_batch(batch)\n",
    "pred = model([x, tf_a], training=False)\n",
    "print(\"ID: Label\")\n",
    "for i,j in zip(data.primary_id, pred):\n",
    "    print(\"{}:{}\".format(i, tf.math.argmax(j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNvMNnOmBZ_w"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of graphsage_node_classification.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "pytorch-gpu.1-9.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m81"
  },
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc5eadac82f5951e7eb836bb06f3c9df8e6d1eda5537a95773af6c6ed24cb2d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
